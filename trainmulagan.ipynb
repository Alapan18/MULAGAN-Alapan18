{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd C:/Users/alapa/Downloads/MuLA_GAN-main/MuLA_GAN-main/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from models.commons import Weights_Normal, VGG19_PercepLoss\n",
    "from models.MuLA_GAN import MuLA_GAN_Generator, Discriminator\n",
    "from utils.data_utils import Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--cfg_file\", type=str, default=\"configs/train_MuLA-GAN.yaml\")\n",
    "parser.add_argument(\"--epoch\", type=int, default=0, help=\"which epoch to start from\")\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=400, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=2, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0001, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of 1st order momentum\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.99, help=\"adam: decay of 2nd order momentum\")\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training params\n",
    "epoch = args.epoch\n",
    "num_epochs = args.num_epochs\n",
    "batch_size =  args.batch_size\n",
    "lr_rate, lr_b1, lr_b2 = args.lr, args.b1, args.b2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data config file\n",
    "with open(args.cfg_file) as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# get info from config file\n",
    "dataset_name = cfg[\"dataset_name\"] \n",
    "dataset_path = cfg[\"dataset_path\"]\n",
    "channels = cfg[\"chans\"]\n",
    "img_width = cfg[\"im_width\"]\n",
    "img_height = cfg[\"im_height\"] \n",
    "val_interval = cfg[\"val_interval\"]\n",
    "ckpt_interval = cfg[\"ckpt_interval\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"##############\")\n",
    "# create dir for model and validation data\n",
    "samples_dir = os.path.join(\"samples/\", dataset_name)\n",
    "checkpoint_dir = os.path.join(\"checkpoints/\", dataset_name)\n",
    "os.makedirs(samples_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Adv_cGAN = torch.nn.MSELoss()\n",
    "L1_G  = torch.nn.L1Loss() \n",
    "L_vgg = VGG19_PercepLoss() \n",
    "lambda_1, lambda_con = 7, 3 \n",
    "patch = (1, img_height//16, img_width//16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = MuLA_GAN_Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    Adv_cGAN.cuda()\n",
    "    L1_G = L1_G.cuda()\n",
    "    L_vgg = L_vgg.cuda()\n",
    "    Tensor = torch.cuda.FloatTensor    \n",
    "else:\n",
    "    Tensor = torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_rate, betas=(lr_b1, lr_b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_rate, betas=(lr_b1, lr_b2))\n",
    "\n",
    "## Data pipeline\n",
    "transforms_ = [\n",
    "    transforms.Resize((img_height, img_width), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    Dataloader(dataset_path, dataset_name, transforms_=transforms_),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of samples in dataloader: {len(dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Training pipeline\n",
    "for epoch in range(epoch+1, num_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Model inputs\n",
    "        imgs_distorted = Variable(batch[\"A\"].type(Tensor))\n",
    "        imgs_good_gt = Variable(batch[\"B\"].type(Tensor))\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((imgs_distorted.size(0), *patch))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((imgs_distorted.size(0), *patch))), requires_grad=False)\n",
    "\n",
    "        ## Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        imgs_fake = generator(imgs_distorted)\n",
    "        pred_real = discriminator(imgs_good_gt, imgs_distorted)\n",
    "        loss_real = Adv_cGAN(pred_real, valid)\n",
    "        pred_fake = discriminator(imgs_fake, imgs_distorted)\n",
    "        loss_fake = Adv_cGAN(pred_fake, fake)\n",
    "        loss_D = 0.5 * (loss_real + loss_fake) * 10.0 \n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        ## Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        imgs_fake = generator(imgs_distorted)\n",
    "        pred_fake = discriminator(imgs_fake, imgs_distorted)\n",
    "        loss_GAN =  Adv_cGAN(pred_fake, valid) \n",
    "        loss_1 = L1_G(imgs_fake, imgs_good_gt)\n",
    "        loss_con = L_vgg(imgs_fake, imgs_good_gt)\n",
    "        loss_G = loss_GAN + lambda_1 * loss_1  + lambda_con * loss_con \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        ## Print log\n",
    "        if not i%1:\n",
    "            sys.stdout.write(\"\\r[Epoch %d/%d: batch %d/%d] [DLoss: %.3f, GLoss: %.3f, AdvLoss: %.3f]\"\n",
    "                              %(\n",
    "                                epoch, num_epochs, i, len(dataloader),\n",
    "                                loss_D.item(), loss_G.item(), loss_GAN.item(),\n",
    "                               )\n",
    "            )\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "    ## Save model checkpoints\n",
    "    \n",
    "    torch.save(generator.state_dict(), \"checkpoints/%s/generator_%d.pth\" % (dataset_name, epoch))\n",
    "    torch.save(discriminator.state_dict(), \"checkpoints/%s/discriminator_%d.pth\" % (dataset_name, epoch))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
